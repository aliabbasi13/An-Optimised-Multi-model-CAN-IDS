{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------------tunned CNN Model + Attention--------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Multiply, Permute, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# === Load datasets ===\n",
    "attackFree= pd.read_csv('Attack_free new.csv')[0:2369397]\n",
    "doS= pd.read_csv('DoS_Attack_new.csv')[0:656578]\n",
    "fuzzy= pd.read_csv('Fuzzy_Attack_New.csv')[0:591989]\n",
    "impersonation= pd.read_csv('Impersonation_Attack_New.csv')[0:995471]\n",
    "\n",
    "# === Add labels ===\n",
    "for df, label in zip([attack_free, dos, fuzzy, impersonation], [0, 1, 2, 3]):\n",
    "    df['label'] = label\n",
    "\n",
    "df = pd.concat([attack_free, dos, fuzzy, impersonation], ignore_index=True)\n",
    "df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col])\n",
    "\n",
    "# === Convert HEX to numeric ===\n",
    "for col in df.columns[:-1]:\n",
    "    df[col] = df[col].apply(lambda x: int(str(x), 16) if isinstance(x, str) and all(c in '0123456789abcdefABCDEF' for c in str(x)) else x)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# === Feature matrix and labels ===\n",
    "X = df.drop(columns='label').values\n",
    "y = df['label'].values\n",
    "\n",
    "# === Normalize ===\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# === Reshape for Conv1D ===\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# === Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "y_test_cat = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "# === Compute class weights ===\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# === Custom Attention ===\n",
    "def attention_layer(inputs):\n",
    "    attention = Dense(inputs.shape[1], activation='softmax')(inputs)\n",
    "    attention = Permute((2, 1))(attention)\n",
    "    attention = Lambda(lambda x: K.mean(x, axis=1))(attention)\n",
    "    attention = Dense(inputs.shape[-1], activation='sigmoid')(attention)\n",
    "    return Multiply()([inputs, attention])\n",
    "\n",
    "# === Build Model ===\n",
    "input_layer = Input(shape=(X.shape[1], 1))\n",
    "x = Conv1D(96, kernel_size=3, activation='relu')(input_layer)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = attention_layer(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(96, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "train_start = datetime.now()\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    class_weight=class_weights,\n",
    "   callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "train_end = datetime.now()\n",
    "print(f\" Total Training Time: {train_end - train_start}\")\n",
    "history_cnn = history\n",
    "\n",
    "\n",
    "# === Plot Training & Validation Loss ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', linestyle='--', linewidth=2)\n",
    "plt.title('Tuned CNN + Attention: Training vs Validation Loss', fontsize=14, weight='bold')\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cnn_bilstm_loss_plot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Evaluate ===\n",
    "test_start = datetime.now()\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "test_end = datetime.now()\n",
    "print(f\"Total Testing Time: {test_end - test_start}\")\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(f\" Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# === Measure latency on a sample batch ===\n",
    "batch_size_latency = 1000  \n",
    "X_latency_sample = X_test[:batch_size_latency]\n",
    "\n",
    "_ = model.predict(X_latency_sample)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "_ = model.predict(X_latency_sample)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total and per-sample latency\n",
    "total_latency = end_time - start_time\n",
    "average_latency_ms = (total_latency / batch_size_latency) * 1000\n",
    "\n",
    "print(f\"\\n Inference Time for {batch_size_latency} samples: {total_latency:.4f} seconds\")\n",
    "print(f\" Average Latency per Sample: {average_latency_ms:.4f} ms\")\n",
    "\n",
    "# === Predictions & Report ===\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(\"\\n Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion Matrix - Tuned CNN + Attention Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === TPR and TNR Calculation ===\n",
    "labels = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']\n",
    "TPR = []\n",
    "TNR = []\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    tp = cm[i, i]\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    tn = cm.sum() - (tp + fn + fp)\n",
    "\n",
    "    tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    TPR.append(tpr)\n",
    "    TNR.append(tnr)\n",
    "\n",
    "# === Print TPR and TNR per class ===\n",
    "print(\"\\n True Positive Rate (Recall) and True Negative Rate (Specificity):\")\n",
    "for i, cls in enumerate(labels):\n",
    "    print(f\"{cls}:\\n  TPR (Recall) = {TPR[i]:.4f}\\n  TNR (Specificity) = {TNR[i]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# === TPR and FPR Calculation ===\n",
    "labels = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']\n",
    "TPR = []\n",
    "FPR = []\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    tp = cm[i, i]\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    tn = cm.sum() - (tp + fn + fp)\n",
    "\n",
    "    tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "\n",
    "    TPR.append(tpr)\n",
    "    FPR.append(fpr)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n TPR (Recall) and FPR (Fall-out) per class:\")\n",
    "for i, cls in enumerate(labels):\n",
    "    print(f\"{cls}:\\n   TPR (Recall) = {TPR[i]:.4f}\\n   FPR = {FPR[i]:.4f}\")\n",
    "\n",
    "\n",
    "## === AUROC Calculation ===    \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Binarize the test labels\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Predict class probabilities\n",
    "y_score = model.predict(X_test)\n",
    "\n",
    "# Compute AUROC for each class\n",
    "auroc_per_class = roc_auc_score(y_test_bin, y_score, average=None)\n",
    "\n",
    "# Print AUROC for each class\n",
    "class_names = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']\n",
    "print(\"\\n AUROC Score per Class:\")\n",
    "for i, score in enumerate(auroc_per_class):\n",
    "    print(f\"{class_names[i]}: AUROC = {score:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
